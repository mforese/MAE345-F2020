{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Following (50 pts)\n",
    "\n",
    "In this notebook we'll show how you can follow an object with JetBot!  We'll use a pre-trained neural network\n",
    "that was trained on the [COCO dataset](http://cocodataset.org) to detect 90 different common objects.  These include\n",
    "\n",
    "* Person (index 0)\n",
    "* Cup (index 47)\n",
    "\n",
    "and many others (you can check [this file](https://github.com/tensorflow/models/blob/master/research/object_detection/data/mscoco_complete_label_map.pbtxt) for a full list of class indices).  The model is sourced from the [TensorFlow object detection API](https://github.com/tensorflow/models/tree/master/research/object_detection),\n",
    "which provides utilities for training object detectors for custom tasks also!  Once the model is trained, we optimize it using NVIDIA TensorRT on the Jetson Nano GPU.\n",
    "\n",
    "This makes the network very fast, capable of real-time execution on Jetson Nano!  We won't run through all of the training and optimization steps in this notebook though. The goal here is to demonstrate what one can do with neural networks. In the final project, you will get a chance to train neural networks for obstacle avoidance and navigation. \n",
    "\n",
    "## Important Tips ##\n",
    "\n",
    "* For this assignment, you will want to use the long extension cable that was recently shipped to you. This cable is 10 feet long and should give the JetBot enough room to move around. \n",
    "- It will be helpful to work in a well-lit environment if possible. This will make the object tracking easier for the JetBot.\n",
    "- Be careful with the JetBot! If possible, it would be helpful to find an area to work in that allows the JetBot to roam around freely. The JetBot moves quite fast and we recommend avoiding collisions. Given that we only have a short amount of time left in the semester we likely wonâ€™t be able to ship replacement parts, so please be as careful as possible. \n",
    "- You may want to angle the camera of the JetBot up a little bit so it can see the object you are following more easily (instead of being pointed in a way that only allows the camera to see the ground).\n",
    "\n",
    "Anyways, let's get started!  First, we'll want to import the ``ObjectDetector`` class. You will also need to download the pre-trained neural network for object detection onto your laptop/desktop and then upload it to the JetBot. You can download the pre-trained neural network from here:\n",
    "\n",
    "https://drive.google.com/file/d/1KjlDMRD8uhgQmQK-nC2CZGHFTbq4qQQH/view\n",
    "\n",
    "Once you have downloaded the file, please upload it to the JetBot (you can just drag and drop the file).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute detections on single camera image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from jetbot import ObjectDetector\n",
    "\n",
    "model = ObjectDetector('ssd_mobilenet_v2_coco.engine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you are interested in some details (feel free to ignore if not): internally, the ``ObjectDetector`` class uses the TensorRT Python API to execute the engine that we provide.  It also takes care of preprocessing the input to the neural network, as\n",
    "well as parsing the detected objects.  Right now it will only work for engines created using the ``jetbot.ssd_tensorrt`` package. That package has the utilities for converting\n",
    "the model from the TensorFlow object detection API to an optimized TensorRT engine.\n",
    "\n",
    "Next, let's initialize our camera.  Our detector takes 300x300 pixel input, so we'll set this when creating the camera.\n",
    "\n",
    "> Internally, the Camera class uses GStreamer to take advantage of Jetson Nano's Image Signal Processor (ISP).  This is super fast and offloads\n",
    "> a lot of the resizing computation from the CPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetbot import Camera\n",
    "\n",
    "camera = Camera.instance(width=300, height=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's execute our network using some camera input.  By default the ``ObjectDetector`` class expects ``bgr8`` format that the camera produces.  However,\n",
    "you could override the default pre-processing function if your input is in a different format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detections = model(camera.value)\n",
    "\n",
    "print(detections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are any COCO objects in the camera's field of view, they should now be stored in the ``detections`` variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display detections in text area\n",
    "\n",
    "We'll use the code below to print out the detected objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import ipywidgets.widgets as widgets\n",
    "\n",
    "detections_widget = widgets.Textarea()\n",
    "\n",
    "detections_widget.value = str(detections)\n",
    "\n",
    "display(detections_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the label, confidence, and bounding box of each object detected in each image.  There's only one image (our camera) in this example. \n",
    "\n",
    "\n",
    "To print just the first object detected in the first image, we could call the following\n",
    "\n",
    "> This may throw an error if no objects are detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_number = 0\n",
    "object_number = 0\n",
    "\n",
    "print(detections[image_number][object_number])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control robot to follow central object\n",
    "\n",
    "Now we want our robot to follow an object of the specified category (e.g., person, etc.).  To do this we'll do the following\n",
    "\n",
    "1.  Detect objects matching the specified class\n",
    "2.  Select object closest to center of camera's field of vision; this is the 'target' object\n",
    "3.  Steer robot towards target object; otherwise stop\n",
    "\n",
    "We'll also define the robot's speed and a simple turning controller that will control how fast the robot turns based off the distance between the target object and the center of the robot's field of view. \n",
    "\n",
    "First, let's define some scripts that will process the images from the JetBot. \n",
    "\n",
    "### Task 1 (10 pts) ###\n",
    "\n",
    "Fill in the function \"closest_detection\" below. This should find the detected object that is closest to the center of the image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda') # This makes the computations happen on the JetBot's GPU\n",
    "\n",
    "mean = 255.0 * np.array([0.485, 0.456, 0.406])\n",
    "stdev = 255.0 * np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "normalize = torchvision.transforms.Normalize(mean, stdev)\n",
    "\n",
    "# Widget for displaying camera feed\n",
    "image_widget = widgets.Image(format='jpeg', width=300, height=300)\n",
    "\n",
    "# Define function for doing some basic pre-processing on images\n",
    "def preprocess(camera_value):\n",
    "    global device, normalize\n",
    "    x = camera_value\n",
    "    x = cv2.resize(x, (224, 224))\n",
    "    x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n",
    "    x = x.transpose((2, 0, 1))\n",
    "    x = torch.from_numpy(x).float()\n",
    "    x = normalize(x)\n",
    "    x = x.to(device)\n",
    "    x = x[None, ...]\n",
    "    return x\n",
    "\n",
    "from jetbot import bgr8_to_jpeg\n",
    "import time\n",
    "\n",
    "width = int(image_widget.width)\n",
    "height = int(image_widget.height)\n",
    "\n",
    "def detection_center(detection):\n",
    "    \"\"\"Computes the center x, y coordinates of the object\"\"\"\n",
    "    bbox = detection['bbox']\n",
    "    center_x = (bbox[0] + bbox[2]) / 2.0 - 0.5\n",
    "    center_y = (bbox[1] + bbox[3]) / 2.0 - 0.5\n",
    "    return (center_x, center_y)\n",
    "    \n",
    "def norm(vec):\n",
    "    \"\"\"Computes the length of the 2D vector\"\"\"\n",
    "    return np.sqrt(vec[0]**2 + vec[1]**2)\n",
    "\n",
    "def closest_detection(detections):\n",
    "    \"\"\"TODO: Find the detection closest to the image center\"\"\"\n",
    "    closest_detection = None\n",
    "    for det in detections:\n",
    "        # Loop through and find the detection that is closest to the image center\n",
    "        # You can use the detection_center function above to find the center of the detected object\n",
    "        # Note that the origin (i.e., (x,y) = (0,0)) corresponds to the center of the image. So you can\n",
    "        # use the \"norm\" function above to find the detection that is closest to the center.\n",
    "        # Return the det that corresponds to the closest detection to the image center.\n",
    "        # If nothing is detected, return None.\n",
    "    return closest_detection\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now let's initialize our robot so we can control the motors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetbot import Robot\n",
    "\n",
    "robot = Robot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's display control widgets for displaing the camera feed, the label of the object that we want to track (e.g., person = 1), and the speed. The block of code below will also be responsible for making the JetBot follow the object.\n",
    "\n",
    "### Task 2 (20 pts) ###\n",
    "\n",
    "Fill in the block of code below that says \"TODO\" to make the JetBot follow the object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### You don't have to modify this block. But please read and understand ###\n",
    "\n",
    "# Widgets for displaying label of object to be tracked, and the speed\n",
    "label_widget = widgets.IntText(value=1, description='tracked label')\n",
    "speed_widget = widgets.FloatSlider(value=0.25, min=0.0, max=1.0, description='speed')\n",
    "\n",
    "# Display widgets\n",
    "display(widgets.VBox([\n",
    "    widgets.HBox([image_widget]),\n",
    "    label_widget,\n",
    "    speed_widget\n",
    "]))\n",
    "\n",
    "# Define main function that takes in an image and provides appropriate control inputs to the JetBot\n",
    "def execute(change):\n",
    "    image = change['new']\n",
    "        \n",
    "    # compute all detected objects\n",
    "    detections = model(image)\n",
    "    \n",
    "    # draw all detections on image\n",
    "    for det in detections[0]:\n",
    "        # Coordinates of bounding box of object\n",
    "        bbox = det['bbox'] \n",
    "        # Draw bounding box\n",
    "        cv2.rectangle(image, (int(width * bbox[0]), int(height * bbox[1])), (int(width * bbox[2]), int(height * bbox[3])), (255, 0, 0), 2)\n",
    "    \n",
    "    # select detections that match selected class label\n",
    "    matching_detections = [d for d in detections[0] if d['label'] == int(label_widget.value)]\n",
    "    \n",
    "    # get detection closest to center of field of view and draw it\n",
    "    det = closest_detection(matching_detections) # This relies on the function you wrote above\n",
    "    if det is not None:\n",
    "        bbox = det['bbox']\n",
    "        cv2.rectangle(image, (int(width * bbox[0]), int(height * bbox[1])), (int(width * bbox[2]), int(height * bbox[3])), (0, 255, 0), 5)\n",
    "    \n",
    "        \n",
    "    # If nothing is detected, stop\n",
    "    if det is None:\n",
    "        robot.forward(float(0.0))\n",
    "        \n",
    "    ###################################\n",
    "        \n",
    "    ############ TODO: Write code that will make the robot follow the object ############\n",
    "    \n",
    "    # otherwsie steer towards target\n",
    "    else:\n",
    "        # move robot forward and steer proportional to target's x-distance from center\n",
    "        center = detection_center(det)\n",
    "        commanded_speed = speed_widget.value\n",
    "        \n",
    "        # Fill in your controller here. \n",
    "        # You can use the robot.set_motors() function to set the motor speeds (see the basic_motion \n",
    "        # notebook in /Notebooks/basic_motion)\n",
    "\n",
    "\n",
    "        \n",
    "    ########################################\n",
    "    \n",
    "    # update image widget\n",
    "    image_widget.value = bgr8_to_jpeg(image)\n",
    "    \n",
    "execute({'new': camera.value})\n",
    "\n",
    "camera.unobserve_all()\n",
    "camera.observe(execute, names='value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome!  If the robot is not blocked you should see boxes drawn around the detected objects in blue.  The target object (which the robot follows) will be displayed in green.\n",
    "\n",
    "The robot should steer towards the target when it is detected.  \n",
    "\n",
    "You can call the code block below to manually disconnect the processing from the camera and stop the robot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "camera.unobserve_all()\n",
    "time.sleep(1.0)\n",
    "robot.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission #\n",
    "\n",
    "Please submit to Gradescope \"HW9: Coding\" a zip including: this notebook Lab9 (30pts), two videos (20pts see below), and Lab10 notebook (50pts).\n",
    "\n",
    "For videos, please submit the following:\n",
    "- (10 pts) A video (e.g., taken from your cellphone) showing the JetBot following you (or any other person). \n",
    "- (10 pts) A video showing the JetBot moving towards a different object (i.e., not a person). For this, you will have to change the \"tracked label\" in the code above to correspond to the object you want the JetBot to follow/move towards. You are welcome to choose any object that is convenient for you. For example, you can place a chair (or whatever object you choose) in front of the JetBot and demonstrate that your code makes the JetBot move towards that object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
